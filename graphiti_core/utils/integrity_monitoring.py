"""
Integrity monitoring service for continuous data validation.

This module provides a monitoring service that continuously watches for
data integrity issues, corruption, and inconsistencies in the knowledge graph.
It performs scheduled integrity checks and reports anomalies.
"""

import asyncio
import logging
import os
import time
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Set, Any, Callable, Awaitable
from dataclasses import dataclass, field
from collections import defaultdict
import json

from .post_save_validation import PostSaveValidator, IntegrityCheckResult
from .centrality_validation import CentralityValidator
from .validation_service import CentralizedValidationService, ValidationReport

logger = logging.getLogger(__name__)


class MonitoringSeverity(Enum):
    """Severity levels for monitoring alerts"""
    INFO = "info"
    WARNING = "warning" 
    ERROR = "error"
    CRITICAL = "critical"


class MonitoringMetric(Enum):
    """Types of metrics tracked by the monitoring service"""
    ENTITY_COUNT = "entity_count"
    EDGE_COUNT = "edge_count"
    ORPHANED_EDGES = "orphaned_edges"
    INVALID_CENTRALITY = "invalid_centrality"
    DUPLICATE_ENTITIES = "duplicate_entities"
    MISSING_EMBEDDINGS = "missing_embeddings"
    TEMPORAL_INCONSISTENCIES = "temporal_inconsistencies"
    VALIDATION_ERRORS = "validation_errors"
    PERFORMANCE_METRICS = "performance_metrics"


@dataclass
class MonitoringAlert:
    """Alert generated by the monitoring service"""
    id: str
    timestamp: datetime
    severity: MonitoringSeverity
    metric: MonitoringMetric
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    entity_ids: List[str] = field(default_factory=list)
    resolved: bool = False
    resolution_timestamp: Optional[datetime] = None


@dataclass
class MonitoringThresholds:
    """Configurable thresholds for monitoring alerts"""
    
    # Entity/edge count change thresholds
    max_entity_count_change_percent: float = 10.0
    max_edge_count_change_percent: float = 15.0
    
    # Data quality thresholds
    max_orphaned_edges_percent: float = 1.0
    max_invalid_centrality_percent: float = 2.0
    max_duplicate_entities_percent: float = 0.5
    max_missing_embeddings_percent: float = 5.0
    
    # Performance thresholds
    max_validation_time_seconds: float = 30.0
    min_throughput_entities_per_second: float = 100.0
    
    # Temporal consistency
    max_temporal_inconsistencies_count: int = 10
    
    @classmethod
    def from_environment(cls) -> 'MonitoringThresholds':
        """Load thresholds from environment variables"""
        
        def get_float_env(key: str, default: float) -> float:
            try:
                return float(os.getenv(key, str(default)))
            except ValueError:
                logger.warning(f"Invalid float value for {key}, using default: {default}")
                return default
        
        def get_int_env(key: str, default: int) -> int:
            try:
                return int(os.getenv(key, str(default)))
            except ValueError:
                logger.warning(f"Invalid int value for {key}, using default: {default}")
                return default
        
        return cls(
            max_entity_count_change_percent=get_float_env('MONITORING_MAX_ENTITY_COUNT_CHANGE_PERCENT', 10.0),
            max_edge_count_change_percent=get_float_env('MONITORING_MAX_EDGE_COUNT_CHANGE_PERCENT', 15.0),
            max_orphaned_edges_percent=get_float_env('MONITORING_MAX_ORPHANED_EDGES_PERCENT', 1.0),
            max_invalid_centrality_percent=get_float_env('MONITORING_MAX_INVALID_CENTRALITY_PERCENT', 2.0),
            max_duplicate_entities_percent=get_float_env('MONITORING_MAX_DUPLICATE_ENTITIES_PERCENT', 0.5),
            max_missing_embeddings_percent=get_float_env('MONITORING_MAX_MISSING_EMBEDDINGS_PERCENT', 5.0),
            max_validation_time_seconds=get_float_env('MONITORING_MAX_VALIDATION_TIME_SECONDS', 30.0),
            min_throughput_entities_per_second=get_float_env('MONITORING_MIN_THROUGHPUT_EPS', 100.0),
            max_temporal_inconsistencies_count=get_int_env('MONITORING_MAX_TEMPORAL_INCONSISTENCIES', 10)
        )


@dataclass
class HealthReport:
    """Overall health report from integrity monitoring"""
    timestamp: datetime
    overall_health: str  # "healthy", "warning", "critical"
    total_alerts: int
    critical_alerts: int
    error_alerts: int
    warning_alerts: int
    metrics: Dict[MonitoringMetric, Any] = field(default_factory=dict)
    alerts: List[MonitoringAlert] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'timestamp': self.timestamp.isoformat(),
            'overall_health': self.overall_health,
            'total_alerts': self.total_alerts,
            'critical_alerts': self.critical_alerts,
            'error_alerts': self.error_alerts,
            'warning_alerts': self.warning_alerts,
            'metrics': {metric.value: value for metric, value in self.metrics.items()},
            'alerts': [
                {
                    'id': alert.id,
                    'timestamp': alert.timestamp.isoformat(),
                    'severity': alert.severity.value,
                    'metric': alert.metric.value,
                    'message': alert.message,
                    'details': alert.details,
                    'entity_ids': alert.entity_ids,
                    'resolved': alert.resolved
                }
                for alert in self.alerts
            ]
        }


class IntegrityMonitoringService:
    """Service for continuous integrity monitoring and alerting"""
    
    def __init__(
        self, 
        driver=None,
        validation_service: Optional[CentralizedValidationService] = None,
        thresholds: Optional[MonitoringThresholds] = None
    ):
        self.driver = driver
        self.validation_service = validation_service or CentralizedValidationService(driver=driver)
        self.thresholds = thresholds or MonitoringThresholds.from_environment()
        
        self.alerts: List[MonitoringAlert] = []
        self.historical_metrics: Dict[MonitoringMetric, List[Dict[str, Any]]] = defaultdict(list)
        
        # Initialize validators
        if driver:
            self.post_save_validator = PostSaveValidator(driver)
            self.centrality_validator = CentralityValidator()
        else:
            self.post_save_validator = None
            self.centrality_validator = CentralityValidator()
        
        # Monitoring configuration
        self.monitoring_enabled = os.getenv('INTEGRITY_MONITORING_ENABLED', 'true').lower() == 'true'
        self.monitoring_interval = int(os.getenv('INTEGRITY_MONITORING_INTERVAL_SECONDS', '300'))  # 5 minutes
        self.alert_retention_hours = int(os.getenv('MONITORING_ALERT_RETENTION_HOURS', '168'))  # 7 days
        
        # Custom check registry
        self.custom_checks: Dict[str, Callable[[], Awaitable[List[MonitoringAlert]]]] = {}
        
        logger.info(f"Integrity monitoring service initialized (enabled: {self.monitoring_enabled})")
    
    def register_custom_check(self, name: str, check_func: Callable[[], Awaitable[List[MonitoringAlert]]]):
        """Register a custom monitoring check"""
        self.custom_checks[name] = check_func
        logger.info(f"Registered custom monitoring check: {name}")
    
    async def check_entity_count_changes(self) -> List[MonitoringAlert]:
        """Check for significant changes in entity count"""
        if not self.driver:
            return []
        
        alerts = []
        
        try:
            async with self.driver.session() as session:
                # Get current entity count
                result = await session.run("MATCH (n:Entity) RETURN count(n) as count")
                record = await result.single()
                current_count = record['count'] if record else 0
                
                # Compare with historical data
                entity_history = self.historical_metrics[MonitoringMetric.ENTITY_COUNT]
                if entity_history:
                    previous_count = entity_history[-1]['value']
                    if previous_count > 0:
                        change_percent = abs(current_count - previous_count) / previous_count * 100
                        
                        if change_percent > self.thresholds.max_entity_count_change_percent:
                            severity = MonitoringSeverity.WARNING if change_percent < 20 else MonitoringSeverity.ERROR
                            alert = MonitoringAlert(
                                id=f"entity_count_change_{int(time.time())}",
                                timestamp=datetime.now(),
                                severity=severity,
                                metric=MonitoringMetric.ENTITY_COUNT,
                                message=f"Entity count changed by {change_percent:.1f}% (from {previous_count} to {current_count})",
                                details={
                                    'previous_count': previous_count,
                                    'current_count': current_count,
                                    'change_percent': change_percent
                                }
                            )
                            alerts.append(alert)
                
                # Record current count
                self.historical_metrics[MonitoringMetric.ENTITY_COUNT].append({
                    'timestamp': datetime.now(),
                    'value': current_count
                })
                
        except Exception as e:
            logger.error(f"Error checking entity count changes: {e}")
            alert = MonitoringAlert(
                id=f"entity_count_error_{int(time.time())}",
                timestamp=datetime.now(),
                severity=MonitoringSeverity.ERROR,
                metric=MonitoringMetric.ENTITY_COUNT,
                message=f"Failed to check entity count: {str(e)}",
                details={'error': str(e)}
            )
            alerts.append(alert)
        
        return alerts
    
    async def check_orphaned_edges(self) -> List[MonitoringAlert]:
        """Check for edges with missing source or target nodes"""
        if not self.driver:
            return []
        
        alerts = []
        
        try:
            async with self.driver.session() as session:
                # Count total edges
                result = await session.run("MATCH ()-[r:RELATES_TO]->() RETURN count(r) as total")
                record = await result.single()
                total_edges = record['total'] if record else 0
                
                if total_edges == 0:
                    return alerts
                
                # Count orphaned edges (edges with missing nodes)
                orphaned_query = """
                MATCH (s)-[r:RELATES_TO]->(t)
                WHERE s.uuid IS NULL OR t.uuid IS NULL OR s.uuid = '' OR t.uuid = ''
                RETURN count(r) as orphaned, collect(r.uuid)[0..10] as sample_edge_ids
                """
                result = await session.run(orphaned_query)
                record = await result.single()
                orphaned_count = record['orphaned'] if record else 0
                sample_edge_ids = record['sample_edge_ids'] if record else []
                
                if orphaned_count > 0:
                    orphaned_percent = orphaned_count / total_edges * 100
                    
                    if orphaned_percent > self.thresholds.max_orphaned_edges_percent:
                        severity = MonitoringSeverity.WARNING if orphaned_percent < 2.0 else MonitoringSeverity.ERROR
                        alert = MonitoringAlert(
                            id=f"orphaned_edges_{int(time.time())}",
                            timestamp=datetime.now(),
                            severity=severity,
                            metric=MonitoringMetric.ORPHANED_EDGES,
                            message=f"Found {orphaned_count} orphaned edges ({orphaned_percent:.1f}% of total)",
                            details={
                                'orphaned_count': orphaned_count,
                                'total_edges': total_edges,
                                'orphaned_percent': orphaned_percent,
                                'sample_edge_ids': sample_edge_ids
                            },
                            entity_ids=sample_edge_ids
                        )
                        alerts.append(alert)
                
        except Exception as e:
            logger.error(f"Error checking orphaned edges: {e}")
            alert = MonitoringAlert(
                id=f"orphaned_edges_error_{int(time.time())}",
                timestamp=datetime.now(),
                severity=MonitoringSeverity.ERROR,
                metric=MonitoringMetric.ORPHANED_EDGES,
                message=f"Failed to check orphaned edges: {str(e)}",
                details={'error': str(e)}
            )
            alerts.append(alert)
        
        return alerts
    
    async def check_centrality_validity(self) -> List[MonitoringAlert]:
        """Check for invalid centrality values"""
        if not self.driver:
            return []
        
        alerts = []
        
        try:
            async with self.driver.session() as session:
                # Count entities with invalid centrality values
                invalid_query = """
                MATCH (n:Entity)
                WHERE 
                    n.degree_centrality < 0 OR n.degree_centrality > 1 OR
                    n.pagerank_centrality < 0 OR n.pagerank_centrality > 1 OR
                    n.betweenness_centrality < 0 OR n.betweenness_centrality > 1 OR
                    n.eigenvector_centrality < 0 OR n.eigenvector_centrality > 1
                RETURN count(n) as invalid_count, collect(n.uuid)[0..10] as sample_entity_ids
                """
                result = await session.run(invalid_query)
                record = await result.single()
                invalid_count = record['invalid_count'] if record else 0
                sample_entity_ids = record['sample_entity_ids'] if record else []
                
                # Get total entity count for percentage calculation
                result = await session.run("MATCH (n:Entity) RETURN count(n) as total")
                record = await result.single()
                total_entities = record['total'] if record else 0
                
                if invalid_count > 0 and total_entities > 0:
                    invalid_percent = invalid_count / total_entities * 100
                    
                    if invalid_percent > self.thresholds.max_invalid_centrality_percent:
                        severity = MonitoringSeverity.WARNING if invalid_percent < 5.0 else MonitoringSeverity.ERROR
                        alert = MonitoringAlert(
                            id=f"invalid_centrality_{int(time.time())}",
                            timestamp=datetime.now(),
                            severity=severity,
                            metric=MonitoringMetric.INVALID_CENTRALITY,
                            message=f"Found {invalid_count} entities with invalid centrality values ({invalid_percent:.1f}% of total)",
                            details={
                                'invalid_count': invalid_count,
                                'total_entities': total_entities,
                                'invalid_percent': invalid_percent,
                                'sample_entity_ids': sample_entity_ids
                            },
                            entity_ids=sample_entity_ids
                        )
                        alerts.append(alert)
                
        except Exception as e:
            logger.error(f"Error checking centrality validity: {e}")
            alert = MonitoringAlert(
                id=f"centrality_validity_error_{int(time.time())}",
                timestamp=datetime.now(),
                severity=MonitoringSeverity.ERROR,
                metric=MonitoringMetric.INVALID_CENTRALITY,
                message=f"Failed to check centrality validity: {str(e)}",
                details={'error': str(e)}
            )
            alerts.append(alert)
        
        return alerts
    
    async def check_duplicate_entities(self) -> List[MonitoringAlert]:
        """Check for potential duplicate entities"""
        if not self.driver:
            return []
        
        alerts = []
        
        try:
            async with self.driver.session() as session:
                # Find entities with identical names (potential duplicates)
                duplicate_query = """
                MATCH (n:Entity)
                WITH n.name as name, collect(n.uuid) as uuids
                WHERE size(uuids) > 1 AND name IS NOT NULL AND name <> ''
                RETURN count(*) as duplicate_groups, 
                       reduce(total = 0, uuids_list IN collect(uuids) | total + size(uuids_list)) as duplicate_entities,
                       collect(uuids)[0..5] as sample_groups
                """
                result = await session.run(duplicate_query)
                record = await result.single()
                duplicate_groups = record['duplicate_groups'] if record else 0
                duplicate_entities = record['duplicate_entities'] if record else 0
                sample_groups = record['sample_groups'] if record else []
                
                # Get total entity count for percentage calculation
                result = await session.run("MATCH (n:Entity) RETURN count(n) as total")
                record = await result.single()
                total_entities = record['total'] if record else 0
                
                if duplicate_entities > 0 and total_entities > 0:
                    duplicate_percent = duplicate_entities / total_entities * 100
                    
                    if duplicate_percent > self.thresholds.max_duplicate_entities_percent:
                        severity = MonitoringSeverity.WARNING if duplicate_percent < 1.0 else MonitoringSeverity.ERROR
                        
                        # Flatten sample entity IDs
                        sample_entity_ids = []
                        for group in sample_groups[:3]:  # Take first 3 groups
                            sample_entity_ids.extend(group[:3])  # Take first 3 entities from each group
                        
                        alert = MonitoringAlert(
                            id=f"duplicate_entities_{int(time.time())}",
                            timestamp=datetime.now(),
                            severity=severity,
                            metric=MonitoringMetric.DUPLICATE_ENTITIES,
                            message=f"Found {duplicate_groups} groups with {duplicate_entities} potentially duplicate entities ({duplicate_percent:.1f}% of total)",
                            details={
                                'duplicate_groups': duplicate_groups,
                                'duplicate_entities': duplicate_entities,
                                'total_entities': total_entities,
                                'duplicate_percent': duplicate_percent
                            },
                            entity_ids=sample_entity_ids
                        )
                        alerts.append(alert)
                
        except Exception as e:
            logger.error(f"Error checking duplicate entities: {e}")
            alert = MonitoringAlert(
                id=f"duplicate_entities_error_{int(time.time())}",
                timestamp=datetime.now(),
                severity=MonitoringSeverity.ERROR,
                metric=MonitoringMetric.DUPLICATE_ENTITIES,
                message=f"Failed to check duplicate entities: {str(e)}",
                details={'error': str(e)}
            )
            alerts.append(alert)
        
        return alerts
    
    async def check_missing_embeddings(self) -> List[MonitoringAlert]:
        """Check for entities missing required embeddings"""
        if not self.driver:
            return []
        
        alerts = []
        
        try:
            async with self.driver.session() as session:
                # Count entities without name embeddings
                missing_query = """
                MATCH (n:Entity)
                WHERE n.name IS NOT NULL AND n.name <> '' AND 
                      (n.name_embedding IS NULL OR size(n.name_embedding) = 0)
                RETURN count(n) as missing_count, collect(n.uuid)[0..10] as sample_entity_ids
                """
                result = await session.run(missing_query)
                record = await result.single()
                missing_count = record['missing_count'] if record else 0
                sample_entity_ids = record['sample_entity_ids'] if record else []
                
                # Get total entity count for percentage calculation
                result = await session.run("MATCH (n:Entity) WHERE n.name IS NOT NULL AND n.name <> '' RETURN count(n) as total")
                record = await result.single()
                total_entities = record['total'] if record else 0
                
                if missing_count > 0 and total_entities > 0:
                    missing_percent = missing_count / total_entities * 100
                    
                    if missing_percent > self.thresholds.max_missing_embeddings_percent:
                        severity = MonitoringSeverity.WARNING if missing_percent < 10.0 else MonitoringSeverity.ERROR
                        alert = MonitoringAlert(
                            id=f"missing_embeddings_{int(time.time())}",
                            timestamp=datetime.now(),
                            severity=severity,
                            metric=MonitoringMetric.MISSING_EMBEDDINGS,
                            message=f"Found {missing_count} entities missing name embeddings ({missing_percent:.1f}% of named entities)",
                            details={
                                'missing_count': missing_count,
                                'total_entities': total_entities,
                                'missing_percent': missing_percent,
                                'sample_entity_ids': sample_entity_ids
                            },
                            entity_ids=sample_entity_ids
                        )
                        alerts.append(alert)
                
        except Exception as e:
            logger.error(f"Error checking missing embeddings: {e}")
            alert = MonitoringAlert(
                id=f"missing_embeddings_error_{int(time.time())}",
                timestamp=datetime.now(),
                severity=MonitoringSeverity.ERROR,
                metric=MonitoringMetric.MISSING_EMBEDDINGS,
                message=f"Failed to check missing embeddings: {str(e)}",
                details={'error': str(e)}
            )
            alerts.append(alert)
        
        return alerts
    
    async def run_monitoring_cycle(self) -> HealthReport:
        """Run a complete monitoring cycle"""
        start_time = time.time()
        cycle_alerts = []
        
        if not self.monitoring_enabled:
            logger.debug("Integrity monitoring is disabled")
            return HealthReport(
                timestamp=datetime.now(),
                overall_health="disabled",
                total_alerts=0,
                critical_alerts=0,
                error_alerts=0,
                warning_alerts=0
            )
        
        logger.info("Starting integrity monitoring cycle")
        
        # Run all monitoring checks
        check_functions = [
            self.check_entity_count_changes,
            self.check_orphaned_edges,
            self.check_centrality_validity,
            self.check_duplicate_entities,
            self.check_missing_embeddings
        ]
        
        # Add custom checks
        for name, check_func in self.custom_checks.items():
            check_functions.append(check_func)
        
        # Execute all checks
        for check_func in check_functions:
            try:
                check_alerts = await check_func()
                cycle_alerts.extend(check_alerts)
            except Exception as e:
                logger.error(f"Error in monitoring check {check_func.__name__}: {e}")
                error_alert = MonitoringAlert(
                    id=f"check_error_{check_func.__name__}_{int(time.time())}",
                    timestamp=datetime.now(),
                    severity=MonitoringSeverity.ERROR,
                    metric=MonitoringMetric.VALIDATION_ERRORS,
                    message=f"Monitoring check {check_func.__name__} failed: {str(e)}",
                    details={'error': str(e), 'check_function': check_func.__name__}
                )
                cycle_alerts.append(error_alert)
        
        # Add new alerts to the alert history
        self.alerts.extend(cycle_alerts)
        
        # Clean up old alerts
        self._cleanup_old_alerts()
        
        # Generate health report
        cycle_time = time.time() - start_time
        active_alerts = [alert for alert in self.alerts if not alert.resolved]
        
        critical_count = len([a for a in active_alerts if a.severity == MonitoringSeverity.CRITICAL])
        error_count = len([a for a in active_alerts if a.severity == MonitoringSeverity.ERROR])
        warning_count = len([a for a in active_alerts if a.severity == MonitoringSeverity.WARNING])
        
        # Determine overall health
        if critical_count > 0:
            overall_health = "critical"
        elif error_count > 0:
            overall_health = "error"
        elif warning_count > 0:
            overall_health = "warning"
        else:
            overall_health = "healthy"
        
        health_report = HealthReport(
            timestamp=datetime.now(),
            overall_health=overall_health,
            total_alerts=len(active_alerts),
            critical_alerts=critical_count,
            error_alerts=error_count,
            warning_alerts=warning_count,
            alerts=cycle_alerts,  # Only new alerts from this cycle
            metrics={
                MonitoringMetric.PERFORMANCE_METRICS: {
                    'cycle_time_seconds': cycle_time,
                    'checks_executed': len(check_functions),
                    'new_alerts_generated': len(cycle_alerts)
                }
            }
        )
        
        logger.info(f"Monitoring cycle completed in {cycle_time:.2f}s - Health: {overall_health} "
                   f"({len(active_alerts)} active alerts: {critical_count} critical, {error_count} error, {warning_count} warning)")
        
        return health_report
    
    def resolve_alert(self, alert_id: str, resolution_note: Optional[str] = None) -> bool:
        """Resolve an alert by marking it as resolved"""
        for alert in self.alerts:
            if alert.id == alert_id and not alert.resolved:
                alert.resolved = True
                alert.resolution_timestamp = datetime.now()
                if resolution_note:
                    alert.details['resolution_note'] = resolution_note
                logger.info(f"Resolved alert {alert_id}: {alert.message}")
                return True
        return False
    
    def get_active_alerts(self, severity_filter: Optional[MonitoringSeverity] = None) -> List[MonitoringAlert]:
        """Get all active (unresolved) alerts, optionally filtered by severity"""
        active = [alert for alert in self.alerts if not alert.resolved]
        if severity_filter:
            active = [alert for alert in active if alert.severity == severity_filter]
        return active
    
    def get_health_summary(self) -> Dict[str, Any]:
        """Get a summary of system health"""
        active_alerts = self.get_active_alerts()
        
        severity_counts = {
            'critical': len([a for a in active_alerts if a.severity == MonitoringSeverity.CRITICAL]),
            'error': len([a for a in active_alerts if a.severity == MonitoringSeverity.ERROR]),
            'warning': len([a for a in active_alerts if a.severity == MonitoringSeverity.WARNING]),
            'info': len([a for a in active_alerts if a.severity == MonitoringSeverity.INFO])
        }
        
        # Calculate overall health score (0-100)
        health_score = 100
        health_score -= severity_counts['critical'] * 25  # Critical alerts heavily impact score
        health_score -= severity_counts['error'] * 10    # Error alerts moderately impact score
        health_score -= severity_counts['warning'] * 2   # Warning alerts slightly impact score
        health_score = max(0, health_score)
        
        return {
            'health_score': health_score,
            'overall_status': 'healthy' if health_score >= 90 else 
                            'warning' if health_score >= 70 else 
                            'error' if health_score >= 50 else 'critical',
            'total_active_alerts': len(active_alerts),
            'severity_breakdown': severity_counts,
            'monitoring_enabled': self.monitoring_enabled,
            'last_check': datetime.now().isoformat()
        }
    
    def _cleanup_old_alerts(self):
        """Remove alerts older than the retention period"""
        cutoff_time = datetime.now() - timedelta(hours=self.alert_retention_hours)
        
        initial_count = len(self.alerts)
        self.alerts = [alert for alert in self.alerts if alert.timestamp > cutoff_time]
        
        cleaned_count = initial_count - len(self.alerts)
        if cleaned_count > 0:
            logger.debug(f"Cleaned up {cleaned_count} old alerts (retention: {self.alert_retention_hours} hours)")
    
    async def start_continuous_monitoring(self):
        """Start continuous monitoring in a background task"""
        if not self.monitoring_enabled:
            logger.info("Continuous monitoring is disabled")
            return
        
        logger.info(f"Starting continuous monitoring (interval: {self.monitoring_interval}s)")
        
        while self.monitoring_enabled:
            try:
                health_report = await self.run_monitoring_cycle()
                
                # Log health status
                if health_report.overall_health in ['critical', 'error']:
                    logger.error(f"System health: {health_report.overall_health} - {health_report.total_alerts} active alerts")
                elif health_report.overall_health == 'warning':
                    logger.warning(f"System health: {health_report.overall_health} - {health_report.total_alerts} active alerts")
                else:
                    logger.info(f"System health: {health_report.overall_health}")
                
                # Wait for next cycle
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"Error in continuous monitoring: {e}")
                await asyncio.sleep(self.monitoring_interval)  # Continue monitoring despite errors


# Global monitoring service instance
_monitoring_service = None

def get_monitoring_service(driver=None) -> IntegrityMonitoringService:
    """Get the global monitoring service instance"""
    global _monitoring_service
    if _monitoring_service is None:
        _monitoring_service = IntegrityMonitoringService(driver=driver)
    return _monitoring_service


async def run_integrity_check(driver) -> HealthReport:
    """Convenience function to run a single integrity monitoring cycle"""
    service = get_monitoring_service(driver=driver)
    return await service.run_monitoring_cycle()


def get_monitoring_config() -> Dict[str, Any]:
    """Get current monitoring configuration"""
    return {
        'enabled': os.getenv('INTEGRITY_MONITORING_ENABLED', 'true').lower() == 'true',
        'interval_seconds': int(os.getenv('INTEGRITY_MONITORING_INTERVAL_SECONDS', '300')),
        'alert_retention_hours': int(os.getenv('MONITORING_ALERT_RETENTION_HOURS', '168')),
        'thresholds': MonitoringThresholds.from_environment().__dict__
    }