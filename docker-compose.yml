services:
  graph:
    image: ghcr.io/oculairmedia/graphiti-api:falkordb-corruption-fixes
    volumes:
      - ./server/graph_service/zep_graphiti.py:/app/server/graph_service/zep_graphiti.py:ro
    ports:
      - 8003:8000
    healthcheck:
      test:
        - CMD
        - python
        - -c
        - import urllib.request;
          urllib.request.urlopen('http://localhost:8000/healthcheck')
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      falkordb:
        condition: service_healthy
      graphiti-centrality-rs:
        condition: service_healthy
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-sk-dummy}
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - FALKORDB_URI=redis://falkordb:6379
      - USE_FALKORDB=true
      - PORT=8000
      # Cerebras configuration
      - USE_CEREBRAS=${USE_CEREBRAS:-false}
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-qwen-3-coder-480b}
      - CEREBRAS_SMALL_MODEL=${CEREBRAS_SMALL_MODEL:-qwen-3-coder-480b}
      # Ollama configuration (used when USE_CEREBRAS=false)
      - USE_OLLAMA=${USE_OLLAMA:-true}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://100.81.139.20:11434/v1}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gemma3:12b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large:latest}
      - USE_OLLAMA_EMBEDDINGS=${USE_OLLAMA_EMBEDDINGS:-true}
      # Dedicated embedding endpoint configuration
      - USE_DEDICATED_EMBEDDING_ENDPOINT=${USE_DEDICATED_EMBEDDING_ENDPOINT:-true}
      - OLLAMA_EMBEDDING_BASE_URL=${OLLAMA_EMBEDDING_BASE_URL:-http://192.168.50.80:11434/v1}
      - OLLAMA_EMBEDDING_API_KEY=${OLLAMA_EMBEDDING_API_KEY:-ollama}
      - EMBEDDING_ENDPOINT_FALLBACK=${EMBEDDING_ENDPOINT_FALLBACK:-true}
      - USE_RUST_CENTRALITY=${USE_RUST_CENTRALITY:-true}
      - RUST_CENTRALITY_URL=${RUST_CENTRALITY_URL:-http://graphiti-centrality-rs:3003}
      - RUST_SERVER_URL=http://graph-visualizer-rust:3000
      - RUST_SEARCH_URL=http://graphiti-search-rs:3004
      - USE_RUST_SEARCH=true
      - ENABLE_CACHE_INVALIDATION=true
      - GRAPHITI_DATA_WEBHOOK_URLS=${GRAPHITI_DATA_WEBHOOK_URLS:-http://graph-visualizer-rust:3000/api/webhooks/data-ingestion,http://host.docker.internal:3000/api/webhooks/data-ingestion}
      # Queue configuration
      - USE_QUEUE_FOR_INGESTION=${USE_QUEUE_FOR_INGESTION:-true}
      - QUEUE_URL=http://graphiti-queued:8080
      - QUEUE_FALLBACK_TO_DIRECT=false
      # Aggressive deduplication configuration
      - DEDUP_SIMILARITY_THRESHOLD=${DEDUP_SIMILARITY_THRESHOLD:-0.6}
      - DEDUP_FUZZY_THRESHOLD=${DEDUP_FUZZY_THRESHOLD:-0.9}
      - ENABLE_AGGRESSIVE_DEDUP=${ENABLE_AGGRESSIVE_DEDUP:-true}
      - DEDUP_NORMALIZE_NAMES=${DEDUP_NORMALIZE_NAMES:-true}
      - DEDUP_EPISODE_INTERVAL=${DEDUP_EPISODE_INTERVAL:-10}
    networks:
      - graphiti_network
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graphiti API
      - homepage.icon=si-graphql
      - homepage.href=http://192.168.50.90:8000
      - homepage.description=Knowledge graph API service
  falkordb:
    image: falkordb/falkordb:latest
    container_name: graphiti-falkordb-1
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - falkordb_data:/data
    environment:
      - FALKORDB_ARGS=--save 60 1 --loglevel warning
    networks:
      - graphiti_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    mem_limit: 4g
    memswap_limit: 4g
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=FalkorDB
      - homepage.icon=si-redis
      - homepage.href=http://192.168.50.90:6379
      - homepage.description=Graph database storage
  graph-visualizer-rust:
    image: ghcr.io/oculairmedia/graphiti-rust-visualizer:dev
    container_name: graphiti-visualizer-rust
    restart: unless-stopped
    ports:
      - 3000:3000
    environment:
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - GRAPH_NAME=graphiti_migration
      - RUST_LOG=graph_visualizer=debug,tower_http=debug
      - NODE_LIMIT=${NODE_LIMIT:-100000}
      - EDGE_LIMIT=${EDGE_LIMIT:-100000}
      - MIN_DEGREE_CENTRALITY=${MIN_DEGREE_CENTRALITY:-0.0}
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-300}
      - CACHE_STRATEGY=${CACHE_STRATEGY:-aggressive}
      - FORCE_FRESH_DATA=${FORCE_FRESH_DATA:-false}
      - REDIS_URL=redis://letta-redis-1:6379/2  # Use DB 2 for visual server cache
    depends_on:
      falkordb:
        condition: service_healthy
    networks:
      - graphiti_network
      - letta_network  # For Redis access
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graph Visualizer (Rust)
      - homepage.icon=si-rust
      - homepage.href=http://192.168.50.90:3000
      - homepage.description=High-performance Rust-based graph visualization
  frontend:
    image: ghcr.io/oculairmedia/graphiti-frontend:dev
    container_name: graphiti-frontend-app
    restart: unless-stopped
    expose:
      - 80
    environment:
      - NODE_ENV=production
      - VITE_WS_URL=ws://192.168.50.90:8088/rust-ws
      - VITE_API_BASE_URL=http://192.168.50.90:8088/api
      - VITE_GRAPHITI_WS_URL=ws://192.168.50.90:8088/ws
    depends_on:
      - graph-visualizer-rust
    networks:
      - graphiti_network
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graphiti Frontend App
      - homepage.icon=si-react
      - homepage.description=React frontend application
  nginx:
    image: nginx:alpine
    container_name: graphiti-nginx
    restart: unless-stopped
    ports:
      - 8088:80
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      - graph-visualizer-rust
    networks:
      - graphiti_network
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graphiti Nginx Proxy
      - homepage.icon=si-nginx
      - homepage.href=http://192.168.50.90:8088
      - homepage.description=Reverse proxy for Graphiti services
  # graphiti-mcp:
  #   build:
  #     context: ./mcp_server
  #     dockerfile: Dockerfile
  #   container_name: graphiti-mcp
  #   restart: unless-stopped
  #   ports:
  #     - "8001:8000" # MCP Server HTTP endpoint
  #   environment:
  #     - GRAPHITI_API_URL=http://graph:8000
  #     - NEO4J_URI=bolt://falkordb:6379
  #     - NEO4J_USER=default
  #     - NEO4J_PASSWORD=
  #     - OPENAI_API_KEY=${OPENAI_API_KEY:-}
  #     - MODEL_NAME=${MODEL_NAME:-qwen2.5:32b}
  #     - SMALL_MODEL_NAME=${SMALL_MODEL_NAME:-qwen2.5:7b}
  #     - OPENAI_BASE_URL=${OPENAI_BASE_URL:-http://host.docker.internal:11434/v1}
  #     - SEMAPHORE_LIMIT=${SEMAPHORE_LIMIT:-5}
  #   depends_on:
  #     graph:
  #       condition: service_healthy
  #   networks:
  #     - graphiti_network
  #   labels:
  #     - "homepage.group=AI Services"
  #     - "homepage.name=Graphiti MCP Server"
  #     - "homepage.icon=si-openai"
  #     - "homepage.href=http://192.168.50.90:8001"
  #     - "homepage.description=Model Context Protocol server for AI assistants"
  #   command: ["uv", "run", "graphiti_mcp_server.py", "--transport", "sse"]

  graphiti-centrality-rs:
    image: ghcr.io/oculairmedia/graphiti-centrality-rs:main
    container_name: graphiti-centrality-rs
    restart: unless-stopped
    ports:
      - 3003:3003 # Rust centrality service
    environment:
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - GRAPH_NAME=graphiti_migration
      - BIND_ADDR=0.0.0.0:3003
      - RUST_LOG=graphiti_centrality=info,debug
    depends_on:
      falkordb:
        condition: service_healthy
    networks:
      - graphiti_network
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:3003/health
      interval: 10s
      timeout: 5s
      retries: 3
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Centrality Service (Rust)
      - homepage.icon=si-rust
      - homepage.href=http://192.168.50.90:3003
      - homepage.description=High-performance centrality calculations with
        native FalkorDB algorithms
  
  # High-performance Rust search service (43x faster than Python)
  graphiti-search-rs:
    # build:
    #   context: ./graphiti-search-rs
    #   dockerfile: Dockerfile
    # Use GitHub Actions build with FalkorDB SDK:
    image: ghcr.io/oculairmedia/graphiti-search-rs:relevance-scoring
    container_name: graphiti-search-rs
    restart: unless-stopped
    ports:
      - 3004:3004 # Rust search service
    environment:
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - GRAPH_NAME=graphiti_migration
      - REDIS_URL=redis://letta-redis-1:6379  # Use separate Redis for cache
      - PORT=3004
      - MAX_CONNECTIONS=200
      - CACHE_TTL=300
      - RUST_LOG=graphiti_search=info,debug
      - RUST_SEARCH_ENABLED=${RUST_SEARCH_ENABLED:-false}  # Feature flag
      - RUST_SEARCH_PERCENTAGE=${RUST_SEARCH_PERCENTAGE:-0}  # A/B testing percentage
      - OLLAMA_BASE_URL=${OLLAMA_EMBEDDING_BASE_URL:-http://192.168.50.80:11434/v1}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large:latest}
    depends_on:
      falkordb:
        condition: service_healthy
    networks:
      - graphiti_network
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:3004/health
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Search Service (Rust)
      - homepage.icon=si-rust
      - homepage.href=http://192.168.50.90:3004
      - homepage.description=High-performance search with 43x speedup
  # Queue service for processing ingestion tasks
  graphiti-queued:
    image: graphiti-queued:falkordb-corruption-fixes
    container_name: graphiti-queued
    restart: unless-stopped
    ports:
      - 8059:8080
    volumes:
      - queued_data:/data
    environment:
      - RUST_LOG=queued=info,debug
    networks:
      - graphiti_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/queues"]
      interval: 10s
      timeout: 3s
      retries: 3
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graphiti Queue Service
      - homepage.icon=si-queue
      - homepage.href=http://192.168.50.90:8080
      - homepage.description=Message queue for processing ingestion tasks

  # Worker service that processes queue messages with corruption fix
  graphiti-worker:
    image: ghcr.io/oculairmedia/graphiti-worker:falkordb-corruption-fixes
    container_name: graphiti-worker-1
    restart: unless-stopped
    environment:
      - QUEUED_URL=http://graphiti-queued:8080
      - FALKORDB_HOST=falkordb
      - FALKORDB_PORT=6379
      - FALKORDB_URI=redis://falkordb:6379
      - USE_FALKORDB=true
      - OPENAI_API_KEY=${OPENAI_API_KEY:-sk-dummy}
      # Cerebras configuration (same as API service)
      - USE_CEREBRAS=${USE_CEREBRAS:-false}
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-qwen-3-coder-480b}
      - CEREBRAS_SMALL_MODEL=${CEREBRAS_SMALL_MODEL:-qwen-3-coder-480b}
      # Ollama configuration (used when USE_CEREBRAS=false)
      - USE_OLLAMA=${USE_OLLAMA:-true}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://100.81.139.20:11434/v1}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gemma3:12b}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large:latest}
      - USE_OLLAMA_EMBEDDINGS=${USE_OLLAMA_EMBEDDINGS:-true}
      # Dedicated embedding endpoint configuration
      - USE_DEDICATED_EMBEDDING_ENDPOINT=${USE_DEDICATED_EMBEDDING_ENDPOINT:-true}
      - OLLAMA_EMBEDDING_BASE_URL=${OLLAMA_EMBEDDING_BASE_URL:-http://192.168.50.80:11434/v1}
      - OLLAMA_EMBEDDING_API_KEY=${OLLAMA_EMBEDDING_API_KEY:-ollama}
      - EMBEDDING_ENDPOINT_FALLBACK=${EMBEDDING_ENDPOINT_FALLBACK:-true}
      - WORKER_COUNT=2
      - BATCH_SIZE=5
      - POLL_INTERVAL=1.0
      - ENABLE_DASHBOARD=false
      - LOG_LEVEL=INFO
    depends_on:
      graphiti-queued:
        condition: service_healthy
      falkordb:
        condition: service_healthy
    networks:
      - graphiti_network
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=Graphiti Worker 1
      - homepage.icon=si-kubernetes
      - homepage.description=Ingestion worker with corruption fix

  # Automated backup service for FalkorDB
  falkordb-backup:
    image: ghcr.io/oculairmedia/graphiti-backup:dev
    container_name: graphiti-falkordb-backup
    restart: unless-stopped
    ports:
      - 8091:8080  # Monitoring dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for container access
      - falkordb_backups:/backups/falkordb            # Backup storage volume
      - backup_status:/var/log                        # Status files for dashboard
    environment:
      - FALKORDB_CONTAINER_NAME=graphiti-falkordb-1
      - BACKUP_DIR=/backups/falkordb
      - BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-7}
      - BACKUP_RETENTION_WEEKLY=${BACKUP_RETENTION_WEEKLY:-4}
      - BACKUP_RETENTION_MONTHLY=${BACKUP_RETENTION_MONTHLY:-3}
      - BACKUP_WEBHOOK_URL=${BACKUP_WEBHOOK_URL:-}
      - RUN_INITIAL_BACKUP=${RUN_INITIAL_BACKUP:-true}
      - ENABLE_DASHBOARD=${ENABLE_DASHBOARD:-true}
      - STATUS_FILE=/var/log/backup_status.json
    depends_on:
      falkordb:
        condition: service_healthy
    networks:
      - graphiti_network
    healthcheck:
      test: ["CMD", "sh", "-c", "pgrep crond && curl -f http://localhost:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      start_period: 30s
      retries: 3
    labels:
      - homepage.group=Knowledge Management
      - homepage.name=FalkorDB Backup Service
      - homepage.icon=si-databricks
      - homepage.href=http://192.168.50.90:8091
      - homepage.description=Automated backup service with monitoring dashboard

networks:
  graphiti_network:
    driver: bridge
  letta_network:
    external: true
volumes:
  falkordb_data: null
  falkordb_backups: null
  backup_status: null
  queued_data: null
